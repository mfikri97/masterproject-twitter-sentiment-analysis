{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Username</th>\n",
       "      <th>Text</th>\n",
       "      <th>Date Created</th>\n",
       "      <th>Number of Likes</th>\n",
       "      <th>Number of Retweet</th>\n",
       "      <th>Number of Replies</th>\n",
       "      <th>Source of Tweet</th>\n",
       "      <th>User ID</th>\n",
       "      <th>Conversation ID</th>\n",
       "      <th>status</th>\n",
       "      <th>Category</th>\n",
       "      <th>translated</th>\n",
       "      <th>translated cleaned</th>\n",
       "      <th>sentiment Scores</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>XavierNaxa</td>\n",
       "      <td>Melihat cara kerajaan mahupun pembangkang meny...</td>\n",
       "      <td>2021-12-26 07:16:38+00:00</td>\n",
       "      <td>3047</td>\n",
       "      <td>2239</td>\n",
       "      <td>50</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>1475002598327394306</td>\n",
       "      <td>1475002598327394306</td>\n",
       "      <td>Main post</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Seeing how the government and the opposition s...</td>\n",
       "      <td>government opposition solved flood issue times...</td>\n",
       "      <td>{'neg': 0.126, 'neu': 0.651, 'pos': 0.223, 'co...</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>XavierNaxa</td>\n",
       "      <td>Penatlah dapat sorang menteri perasan cantik n...</td>\n",
       "      <td>2021-12-19 13:59:40+00:00</td>\n",
       "      <td>10386</td>\n",
       "      <td>6531</td>\n",
       "      <td>57</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>1472567309940129792</td>\n",
       "      <td>1472567309940129792</td>\n",
       "      <td>Main post</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Tired of getting a minister to notice this bea...</td>\n",
       "      <td>tired minister notice beauty</td>\n",
       "      <td>{'neg': 0.333, 'neu': 0.23, 'pos': 0.437, 'com...</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>XavierNaxa</td>\n",
       "      <td>Apakah produk Apple yang berjaya anda miliki p...</td>\n",
       "      <td>2021-12-17 16:42:15+00:00</td>\n",
       "      <td>397</td>\n",
       "      <td>60</td>\n",
       "      <td>293</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>1471883452324597762</td>\n",
       "      <td>1471883452324597762</td>\n",
       "      <td>Main post</td>\n",
       "      <td>Technology</td>\n",
       "      <td>What are the successful Apple products you hav...</td>\n",
       "      <td>successful apple products year</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.441, 'pos': 0.559, 'comp...</td>\n",
       "      <td>0.5859</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>XavierNaxa</td>\n",
       "      <td>Promosi penukaran bateri iPhone 8 dan iPhone 8...</td>\n",
       "      <td>2021-12-14 10:57:18+00:00</td>\n",
       "      <td>921</td>\n",
       "      <td>532</td>\n",
       "      <td>63</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>1470709477196193796</td>\n",
       "      <td>1470709477196193796</td>\n",
       "      <td>Main post</td>\n",
       "      <td>Technology</td>\n",
       "      <td>iPhone 8 and iPhone 8 Plus battery replacement...</td>\n",
       "      <td>iphone iphone battery replacement promotion lo...</td>\n",
       "      <td>{'neg': 0.111, 'neu': 0.741, 'pos': 0.148, 'co...</td>\n",
       "      <td>0.1779</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>XavierNaxa</td>\n",
       "      <td>Apa barangan menarik sempena 12.12 yang telah ...</td>\n",
       "      <td>2021-12-11 16:58:35+00:00</td>\n",
       "      <td>225</td>\n",
       "      <td>28</td>\n",
       "      <td>67</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>1469713235125309445</td>\n",
       "      <td>1469713235125309445</td>\n",
       "      <td>Main post</td>\n",
       "      <td>Technology</td>\n",
       "      <td>What interesting items in conjunction with 12....</td>\n",
       "      <td>interesting items conjunction bought share</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.38, 'pos': 0.62, 'compou...</td>\n",
       "      <td>0.5994</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76582</th>\n",
       "      <td>152259</td>\n",
       "      <td>152259</td>\n",
       "      <td>SuriaArul</td>\n",
       "      <td>@SyedSaddiq Ridiculous</td>\n",
       "      <td>2021-09-29 14:02:13+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>1443214534294966280</td>\n",
       "      <td>1442708708308373509</td>\n",
       "      <td>reply</td>\n",
       "      <td>Politics</td>\n",
       "      <td>@SyedSaddiq Ridiculous</td>\n",
       "      <td>ridiculous</td>\n",
       "      <td>{'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>-0.3612</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76583</th>\n",
       "      <td>152260</td>\n",
       "      <td>152260</td>\n",
       "      <td>rotiPATsegi</td>\n",
       "      <td>@SyedSaddiq Guna untuk sekolah, hospital,  ban...</td>\n",
       "      <td>2021-09-29 15:35:21+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>1443237972267061248</td>\n",
       "      <td>1442708708308373509</td>\n",
       "      <td>reply</td>\n",
       "      <td>Politics</td>\n",
       "      <td>@SyedSaddiq Use it for schools, hospitals, hel...</td>\n",
       "      <td>schools hospitals people beneficial wasteful a...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.674, 'pos': 0.326, 'comp...</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76584</th>\n",
       "      <td>152261</td>\n",
       "      <td>152261</td>\n",
       "      <td>RinMaLiK1</td>\n",
       "      <td>@SyedSaddiq Kebanyakan menteri hanya nak penuh...</td>\n",
       "      <td>2021-09-29 16:36:23+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>1443253331653132288</td>\n",
       "      <td>1442708708308373509</td>\n",
       "      <td>reply</td>\n",
       "      <td>Politics</td>\n",
       "      <td>@SyedSaddiq Most ministers only want to meet m...</td>\n",
       "      <td>ministers meet work people excuse actions immo...</td>\n",
       "      <td>{'neg': 0.242, 'neu': 0.484, 'pos': 0.274, 'co...</td>\n",
       "      <td>-0.1531</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76585</th>\n",
       "      <td>152263</td>\n",
       "      <td>152263</td>\n",
       "      <td>Mrzach17</td>\n",
       "      <td>@SyedSaddiq @EZIARIFFIN Pergi laaaaa tlg rakya...</td>\n",
       "      <td>2021-09-29 23:42:04+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>1443360459726356482</td>\n",
       "      <td>1442708708308373509</td>\n",
       "      <td>reply</td>\n",
       "      <td>Politics</td>\n",
       "      <td>@SyedSaddiq @EZIARIFFIN Let's go to the people...</td>\n",
       "      <td>people people people sick businesses bankrupt ...</td>\n",
       "      <td>{'neg': 0.574, 'neu': 0.426, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.7964</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76586</th>\n",
       "      <td>152265</td>\n",
       "      <td>152265</td>\n",
       "      <td>Belalan60727537</td>\n",
       "      <td>@SyedSaddiq isap ganja lg syok不不 https://t.co/...</td>\n",
       "      <td>2021-09-30 08:50:08+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>1443498383398752259</td>\n",
       "      <td>1442708708308373509</td>\n",
       "      <td>reply</td>\n",
       "      <td>Politics</td>\n",
       "      <td>@SyedSaddiq smoking weed is still shocked 不不 h...</td>\n",
       "      <td>smoking weed shocked</td>\n",
       "      <td>{'neg': 0.535, 'neu': 0.465, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.3182</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76587 rows  18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0.1  Unnamed: 0         Username  \\\n",
       "0                 2           2       XavierNaxa   \n",
       "1                 3           3       XavierNaxa   \n",
       "2                 4           4       XavierNaxa   \n",
       "3                 5           5       XavierNaxa   \n",
       "4                 6           6       XavierNaxa   \n",
       "...             ...         ...              ...   \n",
       "76582        152259      152259        SuriaArul   \n",
       "76583        152260      152260      rotiPATsegi   \n",
       "76584        152261      152261        RinMaLiK1   \n",
       "76585        152263      152263         Mrzach17   \n",
       "76586        152265      152265  Belalan60727537   \n",
       "\n",
       "                                                    Text  \\\n",
       "0      Melihat cara kerajaan mahupun pembangkang meny...   \n",
       "1      Penatlah dapat sorang menteri perasan cantik n...   \n",
       "2      Apakah produk Apple yang berjaya anda miliki p...   \n",
       "3      Promosi penukaran bateri iPhone 8 dan iPhone 8...   \n",
       "4      Apa barangan menarik sempena 12.12 yang telah ...   \n",
       "...                                                  ...   \n",
       "76582                             @SyedSaddiq Ridiculous   \n",
       "76583  @SyedSaddiq Guna untuk sekolah, hospital,  ban...   \n",
       "76584  @SyedSaddiq Kebanyakan menteri hanya nak penuh...   \n",
       "76585  @SyedSaddiq @EZIARIFFIN Pergi laaaaa tlg rakya...   \n",
       "76586  @SyedSaddiq isap ganja lg syok不不 https://t.co/...   \n",
       "\n",
       "                    Date Created  Number of Likes  Number of Retweet  \\\n",
       "0      2021-12-26 07:16:38+00:00             3047               2239   \n",
       "1      2021-12-19 13:59:40+00:00            10386               6531   \n",
       "2      2021-12-17 16:42:15+00:00              397                 60   \n",
       "3      2021-12-14 10:57:18+00:00              921                532   \n",
       "4      2021-12-11 16:58:35+00:00              225                 28   \n",
       "...                          ...              ...                ...   \n",
       "76582  2021-09-29 14:02:13+00:00                0                  0   \n",
       "76583  2021-09-29 15:35:21+00:00                0                  0   \n",
       "76584  2021-09-29 16:36:23+00:00                0                  0   \n",
       "76585  2021-09-29 23:42:04+00:00                0                  0   \n",
       "76586  2021-09-30 08:50:08+00:00                0                  0   \n",
       "\n",
       "       Number of Replies      Source of Tweet              User ID  \\\n",
       "0                     50   Twitter for iPhone  1475002598327394306   \n",
       "1                     57      Twitter Web App  1472567309940129792   \n",
       "2                    293   Twitter for iPhone  1471883452324597762   \n",
       "3                     63   Twitter for iPhone  1470709477196193796   \n",
       "4                     67   Twitter for iPhone  1469713235125309445   \n",
       "...                  ...                  ...                  ...   \n",
       "76582                  0   Twitter for iPhone  1443214534294966280   \n",
       "76583                  0  Twitter for Android  1443237972267061248   \n",
       "76584                  0  Twitter for Android  1443253331653132288   \n",
       "76585                  0  Twitter for Android  1443360459726356482   \n",
       "76586                  0  Twitter for Android  1443498383398752259   \n",
       "\n",
       "           Conversation ID     status    Category  \\\n",
       "0      1475002598327394306  Main post  Technology   \n",
       "1      1472567309940129792  Main post  Technology   \n",
       "2      1471883452324597762  Main post  Technology   \n",
       "3      1470709477196193796  Main post  Technology   \n",
       "4      1469713235125309445  Main post  Technology   \n",
       "...                    ...        ...         ...   \n",
       "76582  1442708708308373509      reply    Politics   \n",
       "76583  1442708708308373509      reply    Politics   \n",
       "76584  1442708708308373509      reply    Politics   \n",
       "76585  1442708708308373509      reply    Politics   \n",
       "76586  1442708708308373509      reply    Politics   \n",
       "\n",
       "                                              translated  \\\n",
       "0      Seeing how the government and the opposition s...   \n",
       "1      Tired of getting a minister to notice this bea...   \n",
       "2      What are the successful Apple products you hav...   \n",
       "3      iPhone 8 and iPhone 8 Plus battery replacement...   \n",
       "4      What interesting items in conjunction with 12....   \n",
       "...                                                  ...   \n",
       "76582                             @SyedSaddiq Ridiculous   \n",
       "76583  @SyedSaddiq Use it for schools, hospitals, hel...   \n",
       "76584  @SyedSaddiq Most ministers only want to meet m...   \n",
       "76585  @SyedSaddiq @EZIARIFFIN Let's go to the people...   \n",
       "76586  @SyedSaddiq smoking weed is still shocked 不不 h...   \n",
       "\n",
       "                                      translated cleaned  \\\n",
       "0      government opposition solved flood issue times...   \n",
       "1                           tired minister notice beauty   \n",
       "2                         successful apple products year   \n",
       "3      iphone iphone battery replacement promotion lo...   \n",
       "4             interesting items conjunction bought share   \n",
       "...                                                  ...   \n",
       "76582                                         ridiculous   \n",
       "76583  schools hospitals people beneficial wasteful a...   \n",
       "76584  ministers meet work people excuse actions immo...   \n",
       "76585  people people people sick businesses bankrupt ...   \n",
       "76586                               smoking weed shocked   \n",
       "\n",
       "                                        sentiment Scores  sentiment  \\\n",
       "0      {'neg': 0.126, 'neu': 0.651, 'pos': 0.223, 'co...     0.2732   \n",
       "1      {'neg': 0.333, 'neu': 0.23, 'pos': 0.437, 'com...     0.2263   \n",
       "2      {'neg': 0.0, 'neu': 0.441, 'pos': 0.559, 'comp...     0.5859   \n",
       "3      {'neg': 0.111, 'neu': 0.741, 'pos': 0.148, 'co...     0.1779   \n",
       "4      {'neg': 0.0, 'neu': 0.38, 'pos': 0.62, 'compou...     0.5994   \n",
       "...                                                  ...        ...   \n",
       "76582  {'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound...    -0.3612   \n",
       "76583  {'neg': 0.0, 'neu': 0.674, 'pos': 0.326, 'comp...     0.4404   \n",
       "76584  {'neg': 0.242, 'neu': 0.484, 'pos': 0.274, 'co...    -0.1531   \n",
       "76585  {'neg': 0.574, 'neu': 0.426, 'pos': 0.0, 'comp...    -0.7964   \n",
       "76586  {'neg': 0.535, 'neu': 0.465, 'pos': 0.0, 'comp...    -0.3182   \n",
       "\n",
       "      sentiment_label  \n",
       "0            Positive  \n",
       "1            Positive  \n",
       "2            Positive  \n",
       "3            Positive  \n",
       "4            Positive  \n",
       "...               ...  \n",
       "76582        Negative  \n",
       "76583        Positive  \n",
       "76584        Negative  \n",
       "76585        Negative  \n",
       "76586        Negative  \n",
       "\n",
       "[76587 rows x 18 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged = pd.read_csv('3 VADERphase.csv')\n",
    "df_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entrepreneur = df_merged[df_merged[\"Category\"] == \"Entrepreneur\"]\n",
    "df_entertainer = df_merged[df_merged[\"Category\"] == \"Entertainer\"]\n",
    "df_technology = df_merged[df_merged[\"Category\"] == \"Technology\"]\n",
    "df_celebrity = df_merged[df_merged[\"Category\"] == \"Celebrity\"]\n",
    "df_politics = df_merged[df_merged[\"Category\"] == \"Politics\"]\n",
    "df_others = df_merged[df_merged[\"Category\"] == \"Fitness and others\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERGED\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Replace NaN values with an empty string\n",
    "df_merged['translated cleaned'] =df_merged['translated cleaned'].fillna('')\n",
    "\n",
    "# Define the pipeline with CountVectorizer, SVM classifier, and parameter grid for hyperparameter tuning\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', SVC()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'clf__kernel': ('linear', 'rbf'),\n",
    "    'clf__C': [0.1, 1, 10,1000],\n",
    "    'clf__gamma': [0.01,0.02,0.03,0.04,0.05,0.06,0.1, 1],\n",
    "}\n",
    "\n",
    "# Define the GridSearchCV object with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5, n_jobs=-1)\n",
    "\n",
    "# Split the data into training and testing sets equally for positive and negative sentiment\n",
    "X_pos = df_merged[df_merged['sentiment'] > 0]['translated cleaned']\n",
    "X_neg = df_merged[df_merged['sentiment'] <= 0]['translated cleaned']\n",
    "y_pos = df_merged[df_merged['sentiment'] > 0]['sentiment'].apply(lambda score: 1)\n",
    "y_neg = df_merged[df_merged['sentiment'] <= 0]['sentiment'].apply(lambda score: 0)\n",
    "\n",
    "X_train_pos, X_test_pos, y_train_pos, y_test_pos = train_test_split(X_pos, y_pos, test_size=0.2, random_state=42, stratify=y_pos)\n",
    "X_train_neg, X_test_neg, y_train_neg, y_test_neg = train_test_split(X_neg, y_neg, test_size=0.2, random_state=42, stratify=y_neg)\n",
    "\n",
    "X_train_merge = pd.concat([X_train_pos, X_train_neg])\n",
    "X_test_merge = pd.concat([X_test_pos, X_test_neg])\n",
    "y_train_merge = pd.concat([y_train_pos, y_train_neg])\n",
    "y_test_merge = pd.concat([y_test_pos, y_test_neg])\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(X_train_merge, y_train_merge)\n",
    "\n",
    "# Print the best hyperparameters and corresponding score\n",
    "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.957370413892153\n",
      "Precision: 0.9629402756508423\n",
      "Recall: 0.970170746759926\n",
      "F1 score: 0.9665419890352002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Fit a final model on the entire training set using the best hyperparameters\n",
    "pipeline.set_params(**grid_search.best_params_)\n",
    "pipeline.fit(X_train_merge, y_train_merge)\n",
    "\n",
    "# Use the final model to make predictions on the test set\n",
    "y_pred_merge = pipeline.predict(X_test_merge)\n",
    "\n",
    "# Evaluate the performance of the final model on the test set\n",
    "accuracy_merge = accuracy_score(y_test_merge, y_pred_merge)\n",
    "precision_merge = precision_score(y_test_merge, y_pred_merge)\n",
    "recall_merge = recall_score(y_test_merge, y_pred_merge)\n",
    "f1_merge = f1_score(y_test_merge, y_pred_merge)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_merge}\")\n",
    "print(f\"Precision: {precision_merge}\")\n",
    "print(f\"Recall: {recall_merge}\")\n",
    "print(f\"F1 score: {f1_merge}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Precision    Recall  F1 Score\n",
      "Label                                  \n",
      "Positive   0.946029  0.930307  0.938103\n",
      "Negative   0.960265  0.969451  0.964836\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Fit a final model on the entire training set using the best hyperparameters\n",
    "pipeline.set_params(**grid_search.best_params_)\n",
    "pipeline.fit(X_train_merge, y_train_merge)\n",
    "\n",
    "# Use the final model to make predictions on the test set\n",
    "y_pred_merge = pipeline.predict(X_test_merge)\n",
    "\n",
    "# Evaluate the performance of the final model on the test set\n",
    "accuracy_merge = accuracy_score(y_test_merge, y_pred_merge)\n",
    "precision_merge = precision_score(y_test_merge, y_pred_merge, average=None)\n",
    "recall_merge = recall_score(y_test_merge, y_pred_merge, average=None)\n",
    "f1_merge = f1_score(y_test_merge, y_pred_merge, average=None)\n",
    "\n",
    "# Create a dictionary to store the metric values\n",
    "metrics_dict = {\"Label\": [\"Positive\", \"Negative\"],\n",
    "                \"Precision\": precision_merge,\n",
    "                \"Recall\": recall_merge,\n",
    "                \"F1 Score\": f1_merge}\n",
    "\n",
    "# Convert the dictionary into a dataframe\n",
    "metrics_df = pd.DataFrame.from_dict(metrics_dict)\n",
    "\n",
    "# Set the \"Label\" column as the index\n",
    "metrics_df.set_index(\"Label\", inplace=True)\n",
    "\n",
    "# Print the dataframe\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv(\"merged_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_merge = pd.DataFrame({'text': X_test_merge, 'vader_label': y_test_merge, 'predicted_label': y_pred_merge})\n",
    "output_merge.to_csv(\"svm_merge.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column in the test data portion of df_merged and assign the predicted labels to it\n",
    "df_merged.loc[y_test_merge.index, 'SVM Predicted Label'] = y_pred_merge\n",
    "\n",
    "# Save the updated DataFrame to a CSV file\n",
    "df_merged.to_csv('updated_df_merged.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TECHNOLOGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fikri\\AppData\\Local\\Temp\\ipykernel_5052\\3846090957.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_technology['translated cleaned'] = df_technology['translated cleaned'].fillna('')\n",
      "C:\\Users\\Fikri\\AppData\\Local\\Temp\\ipykernel_5052\\3846090957.py:23: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  X_train_positive = positive_subset['translated cleaned'][:train_size]\n",
      "C:\\Users\\Fikri\\AppData\\Local\\Temp\\ipykernel_5052\\3846090957.py:24: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  X_test_positive = positive_subset['translated cleaned'][train_size:train_size + test_size]\n",
      "C:\\Users\\Fikri\\AppData\\Local\\Temp\\ipykernel_5052\\3846090957.py:25: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  y_train_positive = positive_subset['sentiment'][:train_size].apply(lambda score: 1 if score > 0 else 0)\n",
      "C:\\Users\\Fikri\\AppData\\Local\\Temp\\ipykernel_5052\\3846090957.py:26: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  y_test_positive = positive_subset['sentiment'][train_size:train_size + test_size].apply(lambda score: 1 if score > 0 else 0)\n",
      "C:\\Users\\Fikri\\AppData\\Local\\Temp\\ipykernel_5052\\3846090957.py:28: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  X_train_negative = negative_subset['translated cleaned'][:train_size]\n",
      "C:\\Users\\Fikri\\AppData\\Local\\Temp\\ipykernel_5052\\3846090957.py:29: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  X_test_negative = negative_subset['translated cleaned'][train_size:train_size + test_size]\n",
      "C:\\Users\\Fikri\\AppData\\Local\\Temp\\ipykernel_5052\\3846090957.py:30: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  y_train_negative = negative_subset['sentiment'][:train_size].apply(lambda score: 1 if score > 0 else 0)\n",
      "C:\\Users\\Fikri\\AppData\\Local\\Temp\\ipykernel_5052\\3846090957.py:31: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  y_test_negative = negative_subset['sentiment'][train_size:train_size + test_size].apply(lambda score: 1 if score > 0 else 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'clf__C': 1000, 'clf__gamma': 0.01, 'clf__kernel': 'rbf', 'vect__ngram_range': (1, 1)}\n",
      "Best score: 0.8402032984130277\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Replace NaN values with an empty string\n",
    "df_technology['translated cleaned'] = df_technology['translated cleaned'].fillna('')\n",
    "\n",
    "# Split the data into positive and negative subsets\n",
    "positive_subset = df_technology[df_technology['sentiment'] > 0]\n",
    "negative_subset = df_technology[df_technology['sentiment'] <= 0]\n",
    "\n",
    "# Shuffle the data to ensure a random split\n",
    "positive_subset = shuffle(positive_subset, random_state=42)\n",
    "negative_subset = shuffle(negative_subset, random_state=42)\n",
    "\n",
    "# Determine the smaller subset size\n",
    "subset_size = min(len(positive_subset), len(negative_subset))\n",
    "\n",
    "# Split the subsets into training and testing sets with equal positive and negative class sizes\n",
    "train_size = int(0.8 * subset_size)\n",
    "test_size = subset_size - train_size\n",
    "\n",
    "X_train_positive = positive_subset['translated cleaned'][:train_size]\n",
    "X_test_positive = positive_subset['translated cleaned'][train_size:train_size + test_size]\n",
    "y_train_positive = positive_subset['sentiment'][:train_size].apply(lambda score: 1 if score > 0 else 0)\n",
    "y_test_positive = positive_subset['sentiment'][train_size:train_size + test_size].apply(lambda score: 1 if score > 0 else 0)\n",
    "\n",
    "X_train_negative = negative_subset['translated cleaned'][:train_size]\n",
    "X_test_negative = negative_subset['translated cleaned'][train_size:train_size + test_size]\n",
    "y_train_negative = negative_subset['sentiment'][:train_size].apply(lambda score: 1 if score > 0 else 0)\n",
    "y_test_negative = negative_subset['sentiment'][train_size:train_size + test_size].apply(lambda score: 1 if score > 0 else 0)\n",
    "\n",
    "X_train_tech = pd.concat([X_train_positive, X_train_negative])\n",
    "X_test_tech = pd.concat([X_test_positive, X_test_negative])\n",
    "y_train_tech = pd.concat([y_train_positive, y_train_negative])\n",
    "y_test_tech = pd.concat([y_test_positive, y_test_negative])\n",
    "\n",
    "# Define the pipeline with CountVectorizer, SVM classifier, and parameter grid for hyperparameter tuning\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', SVC()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'clf__kernel': ('linear', 'rbf'),\n",
    "    'clf__C': [0.1, 1, 10, 1000],\n",
    "    'clf__gamma': [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.1, 1],\n",
    "}\n",
    "\n",
    "# Define the GridSearchCV object with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(X_train_tech, y_train_tech)\n",
    "\n",
    "# Print the best hyperparameters and corresponding score\n",
    "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8205128205128205\n",
      "Precision: 0.8140703517587939\n",
      "Recall: 0.8307692307692308\n",
      "F1 score: 0.8223350253807108\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Fit a final model on the entire training set using the best hyperparameters\n",
    "pipeline.set_params(**grid_search.best_params_)\n",
    "pipeline.fit(X_train_tech, y_train_tech)\n",
    "\n",
    "# Use the final model to make predictions on the test set\n",
    "y_pred_tech = pipeline.predict(X_test_tech)\n",
    "\n",
    "# Evaluate the performance of the final model on the test set\n",
    "accuracy_tech = accuracy_score(y_test_tech, y_pred_tech)\n",
    "precision_tech = precision_score(y_test_tech, y_pred_tech)\n",
    "recall_tech = recall_score(y_test_tech, y_pred_tech)\n",
    "f1_tech = f1_score(y_test_tech, y_pred_tech)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_tech}\")\n",
    "print(f\"Precision: {precision_tech}\")\n",
    "print(f\"Recall: {recall_tech}\")\n",
    "print(f\"F1 score: {f1_tech}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Precision    Recall  F1 Score\n",
      "Label                                  \n",
      "Positive   0.830000  0.851282  0.840506\n",
      "Negative   0.847368  0.825641  0.836364\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Fit a final model on the entire training set using the best hyperparameters\n",
    "pipeline.set_params(**grid_search.best_params_)\n",
    "pipeline.fit(X_train_tech, y_train_tech)\n",
    "\n",
    "# Use the final model to make predictions on the test set\n",
    "y_pred_tech = pipeline.predict(X_test_tech)\n",
    "\n",
    "# Evaluate the performance of the final model on the test set\n",
    "accuracy_tech = accuracy_score(y_test_tech, y_pred_tech)\n",
    "precision_tech = precision_score(y_test_tech, y_pred_tech, average=None)\n",
    "recall_tech = recall_score(y_test_tech, y_pred_tech, average=None)\n",
    "f1_tech = f1_score(y_test_tech, y_pred_tech, average=None)\n",
    "\n",
    "# Create a dictionary to store the metric values\n",
    "metrics_dict = {\"Label\": [\"Positive\", \"Negative\"],\n",
    "                \"Precision\": precision_tech,\n",
    "                \"Recall\": recall_tech,\n",
    "                \"F1 Score\": f1_tech}\n",
    "\n",
    "# Convert the dictionary into a dataframe\n",
    "metrics_df = pd.DataFrame.from_dict(metrics_dict)\n",
    "\n",
    "# Set the \"Label\" column as the index\n",
    "metrics_df.set_index(\"Label\", inplace=True)\n",
    "\n",
    "# Print the dataframe\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv(\"tech_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tech = pd.DataFrame({'text': X_test_tech, 'vader_label': y_test_tech, 'predicted_label': y_pred_tech})\n",
    "output_tech.to_csv(\"svm_tech.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fikri\\AppData\\Local\\Temp\\ipykernel_5052\\2872777365.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_technology.loc[y_test_tech.index, 'SVM Predicted Label'] = y_pred_tech\n"
     ]
    }
   ],
   "source": [
    "# Create a new column in the test data portion of df_merged and assign the predicted labels to it\n",
    "df_technology.loc[y_test_tech.index, 'SVM Predicted Label'] = y_pred_tech\n",
    "\n",
    "# Save the updated DataFrame to a CSV file\n",
    "df_technology.to_csv('updated_df_tech.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POLITICS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fikri\\AppData\\Local\\Temp\\ipykernel_5052\\2725342792.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_politics['translated cleaned'] = df_politics['translated cleaned'].fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'clf__C': 10, 'clf__gamma': 0.01, 'clf__kernel': 'rbf', 'vect__ngram_range': (1, 1)}\n",
      "Best score: 0.9316204554076689\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Replace NaN values with an empty string\n",
    "df_politics['translated cleaned'] = df_politics['translated cleaned'].fillna('')\n",
    "\n",
    "# Split the data into positive and negative sentiment\n",
    "pos_data = df_politics[df_politics['sentiment'] > 0]\n",
    "neg_data = df_politics[df_politics['sentiment'] <= 0]\n",
    "\n",
    "# Shuffle the data and split into training and testing sets with equal amounts of positive and negative sentiment\n",
    "pos_train, pos_test = train_test_split(pos_data, test_size=0.2, random_state=42)\n",
    "neg_train, neg_test = train_test_split(neg_data, test_size=0.2, random_state=42)\n",
    "train = shuffle(pd.concat([pos_train, neg_train]))\n",
    "test = shuffle(pd.concat([pos_test, neg_test]))\n",
    "\n",
    "# Split the training and testing data into X and y\n",
    "X_train_pol = train['translated cleaned']\n",
    "y_train_pol = train['sentiment'].apply(lambda score: 1 if score > 0 else 0)\n",
    "X_test_pol = test['translated cleaned']\n",
    "y_test_pol = test['sentiment'].apply(lambda score: 1 if score > 0 else 0)\n",
    "\n",
    "# Define the pipeline with CountVectorizer, SVM classifier, and parameter grid for hyperparameter tuning\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', SVC()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'clf__kernel': ('linear', 'rbf'),\n",
    "    'clf__C': [0.1, 1, 10, 1000],\n",
    "    'clf__gamma': [0.01, 0.1, 1, 10, 100],\n",
    "}\n",
    "\n",
    "# Define the GridSearchCV object with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(X_train_pol, y_train_pol)\n",
    "\n",
    "# Print the best hyperparameters and corresponding score\n",
    "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9353798710400897\n",
      "Precision: 0.9389349112426035\n",
      "Recall: 0.9513189448441247\n",
      "F1 score: 0.9450863609291246\n"
     ]
    }
   ],
   "source": [
    "# Fit a final model on the entire training set using the best hyperparameters\n",
    "pipeline.set_params(**grid_search.best_params_)\n",
    "pipeline.fit(X_train_pol, y_train_pol)\n",
    "\n",
    "# Use the final model to make predictions on the test set\n",
    "y_pred_pol = pipeline.predict(X_test_pol)\n",
    "\n",
    "# Evaluate the performance of the final model on the test set\n",
    "accuracy_pol = accuracy_score(y_test_pol, y_pred_pol)\n",
    "precision_pol = precision_score(y_test_pol, y_pred_pol)\n",
    "recall_pol = recall_score(y_test_pol, y_pred_pol)\n",
    "f1_pol = f1_score(y_test_pol, y_pred_pol)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_pol}\")\n",
    "print(f\"Precision: {precision_pol}\")\n",
    "print(f\"Recall: {recall_pol}\")\n",
    "print(f\"F1 score: {f1_pol}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Precision    Recall  F1 Score\n",
      "Label                                  \n",
      "Positive   0.930217  0.912955  0.921505\n",
      "Negative   0.938935  0.951319  0.945086\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Fit a final model on the entire training set using the best hyperparameters\n",
    "pipeline.set_params(**grid_search.best_params_)\n",
    "pipeline.fit(X_train_pol, y_train_pol)\n",
    "\n",
    "# Use the final model to make predictions on the test set\n",
    "y_pred_pol = pipeline.predict(X_test_pol)\n",
    "\n",
    "# Evaluate the performance of the final model on the test set\n",
    "accuracy_pol = accuracy_score(y_test_pol, y_pred_pol)\n",
    "precision_pol = precision_score(y_test_pol, y_pred_pol, average=None)\n",
    "recall_pol = recall_score(y_test_pol, y_pred_pol, average=None)\n",
    "f1_pol = f1_score(y_test_pol, y_pred_pol, average=None)\n",
    "\n",
    "# Create a dictionary to store the metric values\n",
    "metrics_dict = {\"Label\": [\"Positive\", \"Negative\"],\n",
    "                \"Precision\": precision_pol,\n",
    "                \"Recall\": recall_pol,\n",
    "                \"F1 Score\": f1_pol}\n",
    "\n",
    "# Convert the dictionary into a dataframe\n",
    "metrics_df = pd.DataFrame.from_dict(metrics_dict)\n",
    "\n",
    "# Set the \"Label\" column as the index\n",
    "metrics_df.set_index(\"Label\", inplace=True)\n",
    "\n",
    "# Print the dataframe\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv(\"pol_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pol = pd.DataFrame({'text': X_test_pol, 'vader_label': y_test_pol, 'predicted_label': y_pred_pol})\n",
    "output_pol.to_csv(\"svm_politic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fikri\\AppData\\Local\\Temp\\ipykernel_5052\\496997231.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_politics.loc[y_test_pol.index, 'SVM Predicted Label'] = y_pred_pol\n"
     ]
    }
   ],
   "source": [
    "# Create a new column in the test data portion of df_merged and assign the predicted labels to it\n",
    "df_politics.loc[y_test_pol.index, 'SVM Predicted Label'] = y_pred_pol\n",
    "\n",
    "# Save the updated DataFrame to a CSV file\n",
    "df_politics.to_csv('updated_df_pol.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENTREPRENEUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fikri\\AppData\\Local\\Temp\\ipykernel_5052\\1217829535.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_entrepreneur['translated cleaned'] = df_entrepreneur['translated cleaned'].fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'clf__C': 1, 'clf__gamma': 0.01, 'clf__kernel': 'linear', 'vect__ngram_range': (1, 1)}\n",
      "Best score: 0.9324212996266228\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Replace NaN values with an empty string\n",
    "df_entrepreneur['translated cleaned'] = df_entrepreneur['translated cleaned'].fillna('')\n",
    "\n",
    "# Split the data into positive and negative sentiment\n",
    "pos_data = df_entrepreneur[df_entrepreneur['sentiment'] > 0]\n",
    "neg_data = df_entrepreneur[df_entrepreneur['sentiment'] <= 0]\n",
    "\n",
    "# Shuffle the data and split into training and testing sets with equal amounts of positive and negative sentiment\n",
    "pos_train, pos_test = train_test_split(pos_data, test_size=0.2, random_state=42)\n",
    "neg_train, neg_test = train_test_split(neg_data, test_size=0.2, random_state=42)\n",
    "train = shuffle(pd.concat([pos_train, neg_train]))\n",
    "test = shuffle(pd.concat([pos_test, neg_test]))\n",
    "\n",
    "# Split the training and testing data into X and y\n",
    "X_train_entrep = train['translated cleaned']\n",
    "y_train_entrep = train['sentiment'].apply(lambda score: 1 if score > 0 else 0)\n",
    "X_test_entrep = test['translated cleaned']\n",
    "y_test_entrep = test['sentiment'].apply(lambda score: 1 if score > 0 else 0)\n",
    "\n",
    "# Define the pipeline with CountVectorizer, SVM classifier, and parameter grid for hyperparameter tuning\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', SVC()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'clf__kernel': ('linear', 'rbf'),\n",
    "    'clf__C': [0.1, 1, 10, 1000],\n",
    "    'clf__gamma': [0.01, 0.1, 1, 10, 100],\n",
    "}\n",
    "\n",
    "# Define the GridSearchCV object with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(X_train_entrep, y_train_entrep)\n",
    "\n",
    "# Print the best hyperparameters and corresponding score\n",
    "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {grid_search.best_score_}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9341438703140831\n",
      "Precision: 0.9492753623188406\n",
      "Recall: 0.9619091326296466\n",
      "F1 score: 0.9555504900843401\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Fit a final model on the entire training set using the best hyperparameters\n",
    "pipeline.set_params(**grid_search.best_params_)\n",
    "pipeline.fit(X_train_entrep, y_train_entrep)\n",
    "\n",
    "# Use the final model to make predictions on the test set\n",
    "y_pred_entrep = pipeline.predict(X_test_entrep)\n",
    "\n",
    "# Evaluate the performance of the final model on the test set\n",
    "accuracy_entrep = accuracy_score(y_test_entrep, y_pred_entrep)\n",
    "precision_entrep = precision_score(y_test_entrep, y_pred_entrep)\n",
    "recall_entrep = recall_score(y_test_entrep, y_pred_entrep)\n",
    "f1_entrep = f1_score(y_test_entrep, y_pred_entrep)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_entrep}\")\n",
    "print(f\"Precision: {precision_entrep}\")\n",
    "print(f\"Recall: {recall_entrep}\")\n",
    "print(f\"F1 score: {f1_entrep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Precision    Recall  F1 Score\n",
      "Label                                  \n",
      "Positive   0.903794  0.852941  0.877632\n",
      "Negative   0.948268  0.967416  0.957746\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Fit a final model on the entire training set using the best hyperparameters\n",
    "pipeline.set_params(**grid_search.best_params_)\n",
    "pipeline.fit(X_train_entrep, y_train_entrep)\n",
    "\n",
    "# Use the final model to make predictions on the test set\n",
    "y_pred_entrep = pipeline.predict(X_test_entrep)\n",
    "\n",
    "# Evaluate the performance of the final model on the test set\n",
    "accuracy_entrep = accuracy_score(y_test_entrep, y_pred_entrep)\n",
    "precision_entrep = precision_score(y_test_entrep, y_pred_entrep, average=None)\n",
    "recall_entrep = recall_score(y_test_entrep, y_pred_entrep, average=None)\n",
    "f1_entrep= f1_score(y_test_entrep, y_pred_entrep, average=None)\n",
    "\n",
    "# Create a dictionary to store the metric values\n",
    "metrics_dict = {\"Label\": [\"Positive\", \"Negative\"],\n",
    "                \"Precision\": precision_entrep,\n",
    "                \"Recall\": recall_entrep,\n",
    "                \"F1 Score\": f1_entrep}\n",
    "\n",
    "# Convert the dictionary into a dataframe\n",
    "metrics_df = pd.DataFrame.from_dict(metrics_dict)\n",
    "\n",
    "# Set the \"Label\" column as the index\n",
    "metrics_df.set_index(\"Label\", inplace=True)\n",
    "\n",
    "# Print the dataframe\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv(\"entrep_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_entrep = pd.DataFrame({'text': X_test_entrep, 'vader_label': y_test_entrep, 'predicted_label': y_pred_entrep})\n",
    "output_entrep.to_csv(\"svm_entrep.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fikri\\AppData\\Local\\Temp\\ipykernel_5052\\2966863844.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_entrepreneur.loc[y_test_entrep.index, 'SVM Predicted Label'] = y_pred_entrep\n"
     ]
    }
   ],
   "source": [
    "# Create a new column in the test data portion of df_merged and assign the predicted labels to it\n",
    "df_entrepreneur.loc[y_test_entrep.index, 'SVM Predicted Label'] = y_pred_entrep\n",
    "\n",
    "# Save the updated DataFrame to a CSV file\n",
    "df_entrepreneur.to_csv('updated_df_entrep.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CELEBRITY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fikri\\AppData\\Local\\Temp\\ipykernel_20240\\4010487705.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_celebrity['translated cleaned'] = df_celebrity['translated cleaned'].fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'C': 100, 'gamma': 0.01, 'kernel': 'rbf', 'probability': True}\n",
      "Best score: 0.8677201531500174\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import shuffle\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Replace NaN values with an empty string\n",
    "df_celebrity['translated cleaned'] = df_celebrity['translated cleaned'].fillna('')\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "param_grid = {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "              'C': [0.1, 1, 10, 100],\n",
    "              'gamma': [0.01, 0.1, 1, 10],\n",
    "              'probability': [True]}\n",
    "\n",
    "\n",
    "\n",
    "# Define the vectorizer\n",
    "vectorizer = TfidfVectorizer(use_idf=True, max_features=10000)\n",
    "\n",
    "# Split the data into positive and negative sentiment\n",
    "pos_data = df_celebrity[df_celebrity['sentiment'] > 0]\n",
    "neg_data = df_celebrity[df_celebrity['sentiment'] <= 0]\n",
    "\n",
    "# Shuffle the data and split into training and testing sets with\n",
    "# equal amounts of positive and negative sentiment\n",
    "pos_train, pos_test = train_test_split(pos_data, test_size=0.2, random_state=42)\n",
    "neg_train, neg_test = train_test_split(neg_data, test_size=0.2, random_state=42)\n",
    "train = shuffle(pd.concat([pos_train, neg_train]))\n",
    "test = shuffle(pd.concat([pos_test, neg_test]))\n",
    "\n",
    "# Split the training and testing data into X and y\n",
    "X_train_celeb = train['translated cleaned']\n",
    "y_train_celeb = train['sentiment'].apply(lambda score: 1 if score > 0 else 0)\n",
    "X_test_celeb = test['translated cleaned']\n",
    "y_test_celeb = test['sentiment'].apply(lambda score: 1 if score > 0 else 0)\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_celeb = vectorizer.fit_transform(X_train_celeb)\n",
    "\n",
    "# Perform oversampling on the training data using SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_celeb_smote, y_train_celeb_smote = sm.fit_resample(X_train_celeb, y_train_celeb)\n",
    "\n",
    "# Define the GridSearchCV object with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(X_train_celeb, y_train_celeb)\n",
    "\n",
    "# Print the best hyperparameters and corresponding score\n",
    "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive    818\n",
       "Negative    242\n",
       "Name: sentiment_label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_counts = df_celebrity['sentiment_label'].value_counts()\n",
    "value_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.59      0.65        49\n",
      "           1       0.88      0.93      0.91       164\n",
      "\n",
      "    accuracy                           0.85       213\n",
      "   macro avg       0.80      0.76      0.78       213\n",
      "weighted avg       0.85      0.85      0.85       213\n",
      "\n",
      "Confusion matrix:\n",
      "              Predicted\n",
      "             |  0  |  1  |\n",
      "True    0   | 29  |  20  |\n",
      "        1   | 11  |  153  |\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Transform the test data using the same vectorizer used for the training data\n",
    "X_test_celeb = vectorizer.transform(X_test_celeb)\n",
    "\n",
    "# Make predictions using the best model obtained from the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_celeb = best_model.predict(X_test_celeb)\n",
    "\n",
    "# Compute the precision, recall, f1-score, and support for positive and negative sentiment\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_test_celeb, y_pred_celeb)\n",
    "print(report)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test_celeb, y_pred_celeb)\n",
    "\n",
    "# Print the classification report with the confusion matrix\n",
    "report = classification_report(y_test_celeb, y_pred_celeb)\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(\"              Predicted\")\n",
    "print(\"             |  0  |  1  |\")\n",
    "print(f\"True    0   | {cm[0][0]}  |  {cm[0][1]}  |\")\n",
    "print(f\"        1   | {cm[1][0]}  |  {cm[1][1]}  |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       0.72      0.59      0.65        49\\n           1       0.88      0.93      0.91       164\\n\\n    accuracy                           0.85       213\\n   macro avg       0.80      0.76      0.78       213\\nweighted avg       0.85      0.85      0.85       213\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metrics_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\CODING TWITTER PROJECT\\4.1 SVM Classifier.ipynb Cell 36\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/CODING%20TWITTER%20PROJECT/4.1%20SVM%20Classifier.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m metrics_df\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39mceleb_metrics.csv\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'metrics_df' is not defined"
     ]
    }
   ],
   "source": [
    "metrics_df.to_csv(\"celeb_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_celeb = pd.DataFrame({'text': X_test_celeb, 'vader_label': y_test_celeb, 'predicted_label': y_pred_celeb})\n",
    "output_celeb.to_csv(\"svm_celeb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column in the test data portion of df_merged and assign the predicted labels to it\n",
    "df_celebrity.loc[y_test_celeb.index, 'SVM Predicted Label'] = y_pred_celeb\n",
    "\n",
    "# Save the updated DataFrame to a CSV file\n",
    "df_celebrity.to_csv('updated_df_celeb.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENTERTAINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fikri\\AppData\\Local\\Temp\\ipykernel_5052\\2395349068.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_entertainer['translated cleaned'] =df_entertainer['translated cleaned'].fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'clf__C': 1, 'clf__gamma': 0.01, 'clf__kernel': 'linear', 'vect__ngram_range': (1, 2)}\n",
      "Best score: 0.9139531317772718\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Replace NaN values with an empty string\n",
    "df_entertainer['translated cleaned'] =df_entertainer['translated cleaned'].fillna('')\n",
    "\n",
    "# Define the pipeline with CountVectorizer, SVM classifier, and parameter grid for hyperparameter tuning\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', SVC()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'clf__kernel': ('linear', 'rbf'),\n",
    "    'clf__C': [0.1, 1, 10, 1000],\n",
    "    'clf__gamma': [0.01, 0.1, 1, 10, 100],\n",
    "}\n",
    "\n",
    "# Define the GridSearchCV object with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5, n_jobs=-1)\n",
    "\n",
    "# Split the data into positive and negative sentiment\n",
    "pos_data = df_entertainer[df_entertainer['sentiment'] > 0]\n",
    "neg_data = df_entertainer[df_entertainer['sentiment'] <= 0]\n",
    "\n",
    "# Shuffle the data and split into training and testing sets with equal amounts of positive and negative sentiment\n",
    "pos_train, pos_test = train_test_split(pos_data, test_size=0.2, random_state=42)\n",
    "neg_train, neg_test = train_test_split(neg_data, test_size=0.2, random_state=42)\n",
    "train = shuffle(pd.concat([pos_train, neg_train]))\n",
    "test = shuffle(pd.concat([pos_test, neg_test]))\n",
    "\n",
    "# Split the training and testing data into X and y\n",
    "X_train_enter = train['translated cleaned']\n",
    "y_train_enter = train['sentiment'].apply(lambda score: 1 if score > 0 else 0)\n",
    "X_test_enter= test['translated cleaned']\n",
    "y_test_enter= test['sentiment'].apply(lambda score: 1 if score > 0 else 0)\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(X_train_enter, y_train_enter)\n",
    "\n",
    "# Print the best hyperparameters and corresponding score\n",
    "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fikri\\AppData\\Local\\Temp\\ipykernel_5052\\765056327.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_entertainer['translated cleaned'] = df_entertainer['translated cleaned'].fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'clf__C': 1, 'clf__gamma': 100, 'clf__kernel': 'linear', 'vect__ngram_range': (1, 1)}\n",
      "Best score: 0.8949494923823466\n"
     ]
    }
   ],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Replace NaN values with an empty string\n",
    "df_entertainer['translated cleaned'] = df_entertainer['translated cleaned'].fillna('')\n",
    "\n",
    "# Define the pipeline with CountVectorizer, SVM classifier, and parameter grid for hyperparameter tuning\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('smote', SMOTE()),\n",
    "    ('clf', SVC()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'clf__kernel': ('linear', 'rbf'),\n",
    "    'clf__C': [0.1, 1, 10, 1000],\n",
    "    'clf__gamma': [0.01, 0.1, 1, 10, 100],\n",
    "}\n",
    "\n",
    "# Define the GridSearchCV object with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5, n_jobs=-1)\n",
    "\n",
    "# Split the data into positive and negative sentiment\n",
    "pos_data = df_entertainer[df_entertainer['sentiment'] > 0]\n",
    "neg_data = df_entertainer[df_entertainer['sentiment'] <= 0]\n",
    "\n",
    "# Shuffle the data and split into training and testing sets with equal amounts of positive and negative sentiment\n",
    "pos_train, pos_test = train_test_split(pos_data, test_size=0.2, random_state=42)\n",
    "neg_train, neg_test = train_test_split(neg_data, test_size=0.2, random_state=42)\n",
    "train = shuffle(pd.concat([pos_train, neg_train]))\n",
    "test = shuffle(pd.concat([pos_test, neg_test]))\n",
    "\n",
    "# Split the training and testing data into X and y\n",
    "X_train_enter = train['translated cleaned']\n",
    "y_train_enter = train['sentiment'].apply(lambda score: 1 if score > 0 else 0)\n",
    "X_test_enter = test['translated cleaned']\n",
    "y_test_enter = test['sentiment'].apply(lambda score: 1 if score > 0 else 0)\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(X_train_enter, y_train_enter)\n",
    "\n",
    "# Print the best hyperparameters and corresponding score\n",
    "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.927159209157128\n",
      "Precision: 0.9412244897959183\n",
      "Recall: 0.9443079443079443\n",
      "F1 score: 0.9427636958299264\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Fit a final model on the entire training set using the best hyperparameters\n",
    "pipeline.set_params(**grid_search.best_params_)\n",
    "pipeline.fit(X_train_enter, y_train_enter)\n",
    "\n",
    "# Use the final model to make predictions on the test set\n",
    "y_pred_enter = pipeline.predict(X_test_enter)\n",
    "\n",
    "# Evaluate the performance of the final model on the test set\n",
    "accuracy_enter = accuracy_score(y_test_enter, y_pred_enter)\n",
    "precision_enter = precision_score(y_test_enter, y_pred_enter)\n",
    "recall_enter = recall_score(y_test_enter, y_pred_enter)\n",
    "f1_enter = f1_score(y_test_enter, y_pred_enter)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_enter}\")\n",
    "print(f\"Precision: {precision_enter}\")\n",
    "print(f\"Recall: {recall_enter}\")\n",
    "print(f\"F1 score: {f1_enter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Precision    Recall  F1 Score\n",
      "Label                                  \n",
      "Positive   0.888889  0.890157  0.889522\n",
      "Negative   0.936885  0.936118  0.936501\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Fit a final model on the entire training set using the best hyperparameters\n",
    "pipeline.set_params(**grid_search.best_params_)\n",
    "pipeline.fit(X_train_enter, y_train_enter)\n",
    "\n",
    "# Use the final model to make predictions on the test set\n",
    "y_pred_enter= pipeline.predict(X_test_enter)\n",
    "\n",
    "# Evaluate the performance of the final model on the test set\n",
    "accuracy_enter = accuracy_score(y_test_enter, y_pred_enter)\n",
    "precision_enter= precision_score(y_test_enter, y_pred_enter, average=None)\n",
    "recall_enter = recall_score(y_test_enter, y_pred_enter, average=None)\n",
    "f1_enter= f1_score(y_test_enter, y_pred_enter, average=None)\n",
    "\n",
    "# Create a dictionary to store the metric values\n",
    "metrics_dict = {\"Label\": [\"Positive\", \"Negative\"],\n",
    "                \"Precision\": precision_enter,\n",
    "                \"Recall\": recall_enter,\n",
    "                \"F1 Score\": f1_enter}\n",
    "\n",
    "# Convert the dictionary into a dataframe\n",
    "metrics_df = pd.DataFrame.from_dict(metrics_dict)\n",
    "\n",
    "# Set the \"Label\" column as the index\n",
    "metrics_df.set_index(\"Label\", inplace=True)\n",
    "\n",
    "# Print the dataframe\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv(\"enter_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_enter = pd.DataFrame({'text': X_test_enter, 'vader_label': y_test_enter, 'predicted_label': y_pred_enter})\n",
    "output_enter.to_csv(\"svm_enter.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fikri\\AppData\\Local\\Temp\\ipykernel_5052\\659843509.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_entertainer.loc[y_test_enter.index, 'SVM Predicted Label'] = y_pred_enter\n"
     ]
    }
   ],
   "source": [
    "# Create a new column in the test data portion of df_merged and assign the predicted labels to it\n",
    "df_entertainer.loc[y_test_enter.index, 'SVM Predicted Label'] = y_pred_enter\n",
    "\n",
    "# Save the updated DataFrame to a CSV file\n",
    "df_entertainer.to_csv('updated_df_enter.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTHERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fikri\\AppData\\Local\\Temp\\ipykernel_5052\\721486208.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_others['translated cleaned'] =df_others['translated cleaned'].fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'clf__C': 10, 'clf__gamma': 0.01, 'clf__kernel': 'rbf', 'vect__ngram_range': (1, 1)}\n",
      "Best score: 0.8922958186591096\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Replace NaN values with an empty string\n",
    "df_others['translated cleaned'] =df_others['translated cleaned'].fillna('')\n",
    "\n",
    "# Define the pipeline with CountVectorizer, SVM classifier, and parameter grid for hyperparameter tuning\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', SVC()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'clf__kernel': ('linear', 'rbf'),\n",
    "    'clf__C': [0.1, 1, 10, 1000],\n",
    "    'clf__gamma': [0.01, 0.1, 1, 10, 100],\n",
    "}\n",
    "\n",
    "# Define the GridSearchCV object with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5, n_jobs=-1)\n",
    "\n",
    "# Split the data into positive and negative sentiment\n",
    "pos_data = df_others[df_others['sentiment'] > 0]\n",
    "neg_data = df_others[df_others['sentiment'] <= 0]\n",
    "\n",
    "# Shuffle the data and split into training and testing sets with equal amounts of positive and negative sentiment\n",
    "pos_train, pos_test = train_test_split(pos_data, test_size=0.2, random_state=42)\n",
    "neg_train, neg_test = train_test_split(neg_data, test_size=0.2, random_state=42)\n",
    "train = shuffle(pd.concat([pos_train, neg_train]))\n",
    "test = shuffle(pd.concat([pos_test, neg_test]))\n",
    "\n",
    "# Split the training and testing data into X and y\n",
    "X_train_others= train['translated cleaned']\n",
    "y_train_others= train['sentiment'].apply(lambda score: 1 if score > 0 else 0)\n",
    "X_test_others= test['translated cleaned']\n",
    "y_test_others= test['sentiment'].apply(lambda score: 1 if score > 0 else 0)\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(X_train_others, y_train_others)\n",
    "\n",
    "# Print the best hyperparameters and corresponding score\n",
    "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9094503756425465\n",
      "Precision: 0.922890103217972\n",
      "Recall: 0.93711467324291\n",
      "F1 score: 0.9299479963291526\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Fit a final model on the entire training set using the best hyperparameters\n",
    "pipeline.set_params(**grid_search.best_params_)\n",
    "pipeline.fit(X_train_others, y_train_others)\n",
    "\n",
    "# Use the final model to make predictions on the test set\n",
    "y_pred_others = pipeline.predict(X_test_others)\n",
    "\n",
    "# Evaluate the performance of the final model on the test set\n",
    "accuracy_others = accuracy_score(y_test_others, y_pred_others)\n",
    "precision_others = precision_score(y_test_others, y_pred_others)\n",
    "recall_others = recall_score(y_test_others, y_pred_others)\n",
    "f1_others = f1_score(y_test_others, y_pred_others)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_others}\")\n",
    "print(f\"Precision: {precision_others}\")\n",
    "print(f\"Recall: {recall_others}\")\n",
    "print(f\"F1 score: {f1_others}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Precision    Recall  F1 Score\n",
      "Label                                  \n",
      "Positive   0.884354  0.859978  0.871996\n",
      "Negative   0.922890  0.937115  0.929948\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Fit a final model on the entire training set using the best hyperparameters\n",
    "pipeline.set_params(**grid_search.best_params_)\n",
    "pipeline.fit(X_train_others, y_train_others)\n",
    "\n",
    "# Use the final model to make predictions on the test set\n",
    "y_pred_others= pipeline.predict(X_test_others)\n",
    "\n",
    "# Evaluate the performance of the final model on the test set\n",
    "accuracy_others = accuracy_score(y_test_others, y_pred_others)\n",
    "precision_others= precision_score(y_test_others, y_pred_others, average=None)\n",
    "recall_others= recall_score(y_test_others, y_pred_others, average=None)\n",
    "f1_others= f1_score(y_test_others, y_pred_others, average=None)\n",
    "\n",
    "# Create a dictionary to store the metric values\n",
    "metrics_dict = {\"Label\": [\"Positive\", \"Negative\"],\n",
    "                \"Precision\": precision_others,\n",
    "                \"Recall\": recall_others,\n",
    "                \"F1 Score\": f1_others}\n",
    "\n",
    "# Convert the dictionary into a dataframe\n",
    "metrics_df = pd.DataFrame.from_dict(metrics_dict)\n",
    "\n",
    "# Set the \"Label\" column as the index\n",
    "metrics_df.set_index(\"Label\", inplace=True)\n",
    "\n",
    "# Print the dataframe\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv(\"others_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_others = pd.DataFrame({'text': X_test_others, 'vader_label': y_test_others, 'predicted_label': y_pred_others})\n",
    "output_others.to_csv(\"svm_others.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fikri\\AppData\\Local\\Temp\\ipykernel_5052\\3208066785.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_others.loc[y_test_others.index, 'SVM Predicted Label'] = y_pred_others\n"
     ]
    }
   ],
   "source": [
    "# Create a new column in the test data portion of df_merged and assign the predicted labels to it\n",
    "df_others.loc[y_test_others.index, 'SVM Predicted Label'] = y_pred_others\n",
    "\n",
    "# Save the updated DataFrame to a CSV file\n",
    "df_others.to_csv('updated_df_others.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9b8631af70f74c0d798f41702a40d0f748abeb6c6f1a069af3db66893b58922"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
